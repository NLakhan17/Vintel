{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b76eec5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b76eec5",
        "outputId": "b19b2aa3-a0c2-4d97-d651-b50a6c6b01f2",
        "scrolled": true,
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.34.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (71.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n",
            "Collecting assemblyai\n",
            "  Downloading assemblyai-0.33.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting httpx>=0.19.0 (from assemblyai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic!=1.10.7,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from assemblyai) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7 in /usr/local/lib/python3.10/dist-packages (from assemblyai) (4.12.2)\n",
            "Collecting websockets>=11.0 (from assemblyai)\n",
            "  Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.19.0->assemblyai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.19.0->assemblyai) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.19.0->assemblyai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.10.7,>=1.7.0->assemblyai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.10.7,>=1.7.0->assemblyai) (2.20.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.19.0->assemblyai) (1.2.2)\n",
            "Downloading assemblyai-0.33.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, h11, httpcore, httpx, assemblyai\n",
            "Successfully installed assemblyai-0.33.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 websockets-13.0.1\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting noisereduce\n",
            "  Downloading noisereduce-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from noisereduce) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from noisereduce) (3.7.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from noisereduce) (0.10.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from noisereduce) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from noisereduce) (4.66.5)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (1.3.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (0.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->noisereduce) (1.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->noisereduce) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->noisereduce) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->noisereduce) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->noisereduce) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->noisereduce) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->noisereduce) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->noisereduce) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->noisereduce) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->noisereduce) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->noisereduce) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->noisereduce) (2024.8.30)\n",
            "Downloading noisereduce-3.0.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: noisereduce\n",
            "Successfully installed noisereduce-3.0.2\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement transformer (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for transformer\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
            "Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.4 rapidfuzz-3.9.7\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.37.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io) (0.37.1)\n",
            "Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow_io\n",
            "Successfully installed tensorflow_io-0.37.1\n"
          ]
        }
      ],
      "source": [
        "!pip install moviepy\n",
        "!pip install assemblyai\n",
        "!pip install librosa\n",
        "!pip install pydub\n",
        "!pip install noisereduce\n",
        "!pip install soundfile\n",
        "!pip install huggingface_hub\n",
        "!pip install transformer\n",
        "!pip install evaluate\n",
        "!pip install jiwer\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow_io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deaf954d-5406-4584-81cd-360c52484421",
      "metadata": {
        "id": "deaf954d-5406-4584-81cd-360c52484421"
      },
      "outputs": [],
      "source": [
        "# pip install moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82285822",
      "metadata": {
        "id": "82285822"
      },
      "outputs": [],
      "source": [
        "from moviepy.editor import *\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import assemblyai as aai\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "import noisereduce as nr\n",
        "import soundfile as sf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Audio Extraction from Video."
      ],
      "metadata": {
        "id": "gK-I_ZhU2ibM"
      },
      "id": "gK-I_ZhU2ibM"
    },
    {
      "cell_type": "code",
      "source": [
        "def video_to_audio():\n",
        "\n",
        "  folder_path = 'C:/Users/Harsh Patel/Desktop/Vintel/videos'\n",
        "  for filename in os.listdir(folder_path):\n",
        "      file_path = os.path.join(folder_path, filename)\n",
        "      if os.path.isfile(file_path) and filename.endswith(('.mp4', '.mov', '.avi', '.mkv')):\n",
        "            print(f\"Processing video file: {filename}\")\n",
        "\n",
        "            # Open the video file and extract audio\n",
        "            videoclip = VideoFileClip(file_path)\n",
        "            audioclip = videoclip.audio\n",
        "\n",
        "            # Write the audio file to mp3 format\n",
        "            audio_output_path = f\"C:/Users/Harsh Patel/Desktop/Vintel/audios/audiofile_{filename.split('.')[0]}.mp3\"\n",
        "            audioclip.write_audiofile(audio_output_path, codec=\"libmp3lame\")\n",
        "            print(f\"Audio saved as: {audio_output_path}\")\n"
      ],
      "metadata": {
        "id": "M4OirE9I2cBd"
      },
      "id": "M4OirE9I2cBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def transcript():\n",
        "\n",
        "#   aai.settings.api_key = \"70383e84e33044fa909ef8ae67a3aef2\"\n",
        "#   transcriber = aai.Transcriber()\n",
        "\n",
        "#   folder_path = 'C:/Users/Harsh Patel/Desktop/Vintel/audios'\n",
        "#   for filename in os.listdir(folder_path):\n",
        "#       file_path = os.path.join(folder_path, filename)\n",
        "#       if os.path.isfile(file_path) and filename.endswith(('.mp3')):\n",
        "\n",
        "#         transcript = transcriber.transcribe(file_path)\n",
        "\n",
        "#     # print(f\"Processing audiofile_Video{i}\")\n",
        "#         transcript= transcript.text\n",
        "#         with open(f'C:/Users/Harsh Patel/Desktop/Vintel/Transcripts/transcript_{filename}.txt', 'w') as f:\n",
        "#                 f.write(transcript)\n",
        "\n"
      ],
      "metadata": {
        "id": "4iU_1mh52c1P"
      },
      "id": "4iU_1mh52c1P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bef55571",
      "metadata": {
        "id": "bef55571"
      },
      "source": [
        "### Reading Audio Signal, and making a HuggingFace Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a590233",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a590233",
        "outputId": "88f1b571-06d5-40fe-cd5d-007088d10772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'audio', 'transcript'],\n",
            "        num_rows: 8\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'audio', 'transcript'],\n",
            "        num_rows: 3\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datasets import Dataset, DatasetDict\n",
        "import soundfile as sf\n",
        "\n",
        "def load_and_preprocess_data(audio_dir, transcript_dir):\n",
        "    data = {'label': [], 'audio': [], 'transcript':[]}\n",
        "\n",
        "    # Process audio files\n",
        "    for index, audio_file in enumerate(os.listdir(audio_dir)):\n",
        "        if audio_file.endswith('.mp3'):  # Adjust for audio file types\n",
        "            file_path = os.path.join(audio_dir, audio_file)\n",
        "\n",
        "            # Assuming preprocess_audio returns the processed audio and sample rate (sr)\n",
        "#             processed_audio, sr = audio_reader(file_path)\n",
        "            processed_audio, sample_rate = librosa.load(file_path, sr=16000, mono=True)\n",
        "\n",
        "            # Append processed audio to the data dictionary\n",
        "            data['audio'].append(processed_audio)\n",
        "\n",
        "    # Process transcript files\n",
        "    for index, transcript_file in enumerate(os.listdir(transcript_dir)):\n",
        "        if transcript_file.endswith('.txt'):  # Adjust for transcript file types\n",
        "            file_path = os.path.join(transcript_dir, transcript_file)\n",
        "\n",
        "            # Read the transcript content and append it\n",
        "            with open(file_path, 'r') as content:\n",
        "                data['transcript'].append(content.read())\n",
        "\n",
        "    # Ensure the label count matches the number of audios/transcripts\n",
        "    num_entries = min(len(data['audio']), len(data['transcript']))  # Match the number of entries\n",
        "    data['audio'] = data['audio'][:num_entries]  # Trim excess audio if necessary\n",
        "    data['transcript'] = data['transcript'][:num_entries]  # Trim excess transcript if necessary\n",
        "\n",
        "    data['label'] = [1] * num_entries  # Create a list of labels (e.g., all set to '1')\n",
        "\n",
        "    return data\n",
        "\n",
        "# Load and preprocess the audio and transcript data\n",
        "audio_data1 = load_and_preprocess_data('/content/sample_data/audios', '/content/sample_data/transcripts')\n",
        "\n",
        "train_dict = {k: v[:8] for k, v in audio_data1.items()}\n",
        "test_dict = {k: v[8:] for k, v in audio_data1.items()}\n",
        "\n",
        "\n",
        "# Convert it into a Hugging Face dataset\n",
        "dataset1 = DatasetDict({\n",
        "    'train': Dataset.from_dict(train_dict),\n",
        "    'test': Dataset.from_dict(test_dict)# Split later into train/test if needed\n",
        "})\n",
        "\n",
        "print(dataset1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a657da22",
      "metadata": {
        "id": "a657da22"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8e789d1",
      "metadata": {
        "id": "d8e789d1"
      },
      "source": [
        "### Speech 2 Text Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O94NvBnMQBmU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O94NvBnMQBmU",
        "outputId": "466a5b62-5ec7-41af-8f6f-caf89e87bc1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription: OVER CAT TEACHER IS GOING TO TEACH YOU COSAN SIMILERTY AND COSAN DISTANCE HOW IT IS USIN TIT A SIENCE WILL LOOK AT SONTYRION THEN WE'LL MOON TO PIES AND GOL LET SE YOUR DID AS IENT IS WORKING FOR SOME FINANCIAL COMPANY WHERE ON A GUGAL DRIVE YOU HAVE BUNGE OF FINANCIAL DOCUMENTS NOW YOU DON'T KNOW FOR WI WHAT IS THE COMPANY ASSOCIATED WITH EACH OF THESE DOCUMENTS BUT WHEN YOU OPENED THE DOCUMEN WEN YOU READ IT YOU CAN KIND OF FIGURE OUT THAT THIS IS PROBABLY ABOUT APPLE WHY BECAUSE IPHON IS MENTIONED SO MANY TIMES SO WHEN YOU ARE READING ABOUT APL FINANCIAL REPORT THEY MIGHT MENTION GELIXY AS WELL BECAUSE THAT' THEIR MAIN COMPETITOR BUT HERE THE RITIO OF IPHON TO GELEXES IPHON HAVE TRI IPHON IS MENTIONED THREE TIMES GILIXIS MENTIONED ONE TIME SO YOU KNOW AL YOU SEE IPHONOCURRENCE MORE MUCH MORE THAN GALAXEY IF YOUR SAMSON DOCUMENT OF COURSE SAMSONCOM NEW DOCUMENT WILL HAVE MORE MENTION OF GALEXY THAN IV ON NOW LET'S SE NEW DOCUMENT COMES IN AND YOU DON'T KNOW WHAT IS THE COMMINE ASSOCIATED WITH IT HERE YOU CAN AGAIN COUNT IFHON AND GALAXY AND YOU FIND THAT IFONI SIX TIMES MENTION IN THE DOCUMEN GALEXIS MENTION TWO TIMES LOOKING AT THE SITUATION AS I DATE AS I INTIST YOU CAN FIGURE OR A SYMPLE FORMULA THAT VENEVER RASHO OF IVONDO GALAXEY IS THREE TO ONE IT SHOULD BE AN APPL DOCUMENT SO NOW YOU CAN AUTO ANOTIDE APPLE AS A COMPANY THIS IS A COMMON PROBLEM IN FINANCIAL INSTITUTE WHERE YOU HAVE A DOCUMENT AND YOU WON'T DO DAG I S OM MATTAR DIT A DO IT AND WE CAN USE SOME ALTOMATION SOME RULE SOME COURDING TO OTTO ENOTATE YOU KNOW YOU CAN MANUALLY ANNODATE ED BUT USING THIS PARTICLE A FORMULA YOU CAN AUTOMETE THA NODATION PROCESS UNFORTUNATELY THINGS IN REAL LIFE ARE QUITE DIFFERENT IN OUR DOCUMENT YOU KNOW THERE MIGHT BE MENTIONED OF I PER GOGOLPIXEL IS ANOTHER COMPETITER SO NOW HOW DO YOU COME UP WITH YOUR FORMULA MAYBE YOU SAY OQUE AFONTU GELEXERISHU HAS TO BE THREE TO ONE AND I ON TO PIXLRESHO HAS TO BE THREE TO ONE AND I BER TO PICTURE RESU HAS TO BE TWO TO ONE WELL THAT'S TO COMPLICATED MY BABY GOT CONFUSED WELL WECTOR MATHEMATICS COMBS AT RISCUE THIS THING CAN BE PRESENTED AS A VECTOR HERE AND USING VECTOR MATHEMATICS WE CAN FIGURE OUR DOCUMENT SIMILARITY LET ME GO BACK TO THE SIMPLE CASE THAT WE SAW BEFORE HARDYO REPRESENT THIS AS A WECTOR WHEN EXEX IS I ARRIVE ON WORDGOUNT WYAXIS AS GELAXY WORD WURGON SO YOU SEE THREE TO ONE AND THIS IS THE VACTOR WECTOR HAS MAGNITUD AND DIRECTION BOTH AND WHEN I AM MY YELLOW DOCUMENT I HAVE ONLY SIX GALEXES TOO NOW LOOK AT THE ANGLE BETWEEN THESE DUERROS BLUE AND YELLOW THE ANGLE IS ZIRO SO THE ANGLE DETERMINES THE DOCUMENT SIMILARITY IF THE ANGLE I ZU IT MEANS DOCUMENTS ARE VERY SIMILAR YOU MIGHT HAVE ANOTHER GREEN DOCUMENT YOU KNIFE ON FI GELEX  ONE TIME HERE THE ANGLE IS STILL NOT THAT HIGH UNITES A LITTLE BIT OF AN ANGLE SHE CAN SAY THESE DOCUMENTS ARE STILL SIMILAR BUT WHEN YOU HAVE A SAMSON DOCUMENT WHERE IPHONIS MENTIONED ONLY ONCE GELEXIS FOR YOU SEE THE ANGLE IS MUCH BIGGER SO THEN YOU HAVE A BIGGER ANGLE YOU CAN SAY THESE DOCUMENTS ARE NOT SIMILAR MEANING IF FOR YELEL EROINO COMPNISAPPLE FOR BLUE ARROW I CAN SAY COMPRIES DEAF DEFINITELY NOT APPLE WORSIS WI AVE A GREEN ARROW WHERE THE TITAR TO ENGLE IS MUTS YOUO MUCH CLOSER TO YELLOW ARROW YOU CAN SEE GREEN AND YELLOW DOCUMENTS ARE SIMILAR WHERE AS BLUE IS NOT SIMILAR LET' SAE THE ANGLE BETWEEN THESE TWO HIS SEVENTEEN DEGREE YOU CAN USE THIS ANGLE TO DEFINE DOCUMENT SIMILARITY ONE THING YOU CAN SAY IS DOCUMENT SIMILARITIES HIS IS SEVENTEEN DEGREE  I MEAN WHEN I SAY THAT IT DOESN'T SOUND THAT OBVIOUS OR INTUTIVE IT'S NOT A GOOD WAY TO REPRESENT SIMILARITY WHAT IF I CAN PRESEN THE SIMILARITY BETWEEN SOME NUMBER BETWEEN LESESIRODO ON WHEN I SAID OCCUMENT SIMILAR TISPOINT NINE WHICH MEANS THEY'RE NINETY PERSON SIMMILERUNO IF THE DOCUMENT SIMILARIT IS ONE THEY'RE A VERY SIMILAR IF IT IS ZERO THEYARE VERY DIFFERENT OKE SO HOW CAN YOU TRANSFORM THIS DEGRE ANGLE INTO ARANGE BECAUSE THAT RIN SEEMS LIKE A GOOD CONVENTION YOU JUST AKE KOSEIN IF YOU DON'T KNOWBER SEIN KOSEIN I MADE A PRIVI VIDIO PREVIOUSLY IN THE SAME SERIOUS WATCH THAT BUT KOSIN OF SEVENTEEN IS BOY NINETY FIVE NOW SEE MY CONVENTION IS MUCH MORE OBVIOUS MUCH MORE EASYAT I CAN SAY DOCUMENT SIMILARITY HERE BETWEEN GREEN AND YELLOW VECTOR IS POIN NINETY FIVE HOORAY ALL RIGHT NOW CORSI AN SIMILARITY IS NOTHING BUT A COSIGN OF AN ANGLE BETWEEN THE TWO WACTORS AND THE GREAT PART ABOUT MATH IS RELOOK THAT VERY SIMPLE SINARIO OF TWO DEMISSON VECTOR IN REAL LIFE YOU WILL HAVE A HUNDRED DIMENSION VECTOR AND THESE DIMENSIONS ARE BYSICALLY THE FEATURES SO YOUR DOCUMENT MIGHT HAVE A HUNDRED FEATURES AND IFYOU CAN PRISON THOSE AS A VECTOR AND YOU CAN STILL DO THE MADSI THAT'S THE BEAUTY OF MAN YOU CANNOT VISULISE UNDER DIMENSION BUT THE MAT WILL CONTINUE TO WOR COGE THE ACADEMIC FORMULA IS BETWEEN HE AND BE THESE ARE THE TWO RECTORS AND THE COSINE SIMILARTY BETWEEN THESE TWO RECTORS IS LORD PRODUCT WHICH IS AD OR B DIVIED BY MAGNUTROVE MAGNUTROVSBI AN E DART BE AS USUALLY BE COS TITASO YOU KNOW IT IS GOSIN SIMILARDIS NOTHING BUT GOSTIDA ALL RIGHT NOW IF YOU HAVE ARROWS POINTING IN THE SAME DIRECTION THEN THE COSIN SIMILARITY IS ONE WHICH MEANS THOSE ARROWS THOSE VECTORS THOSE DOCUMENTS ARE QUITE SIMILAR IF THEY ARE AT NINETY DEGREES MEANS THE SIMILARLY ZERO THEY ARE VERY DIFFERENT AND IF IT IS ONE EHTY DEGREE SIMILAR IT IS MINAS ONE WHICH MEETS THE REPRESENT OPPOSETE CON AR CONCEPT NOW COSIIN DISTANCE IS OF VERY SIMBLE CONCEPT JUST IT IT I USED TO REPRESENT THE SAME THING ITS ONE MINUS COSEIGN SIMILARITY SO HERE WEN TWO ARROWS ARE POINTING IN THE SAME DIRECTION WEN YOU'RE TALKING BUT SEEM RECTOR THE COSIN DISTANCE WILL BE ZIRO SO WHEN YOU SAID DISTANCY ZELO WHICH MEANS THEY ARE SIMILAR THEY ARE CLOSER THAT ONLY ADA BEHIND COURSE AND DISTANCE WHEN YOU HORE TO WACTORS AT NINETY DEGREES COSAN DISTANCE IS ONE WHICH MEANS THERE VERY DIFFERENT AND COFENDISTANCES OR REPRESENTED ONLY IN A POSITIVE SPACE THAS WHAL AND I'M NOT TALKING OVORD AT ONE EDYDYGREE GIS THAT WE SAW IN A PREVIOUS LIDE LESS WRITE THAN BILTEN GORNA WE'LL BE USING PITONS AS KILLON MORE DILFORT IN PORTING COS AND SIMILARLY MATTERD HERE AND WE KNEW HOW THIS MATTER IT EXPECTS TWO ACTORS AND THOSE TWO ACTORS ARE GOING TO BE THE CASE THAT WE LOOKED AT FOR APODOCUMEN WAS THREE ONE AND SIX AND TWO SO LET'S SEE WHAT IS A FAMILIARITY BETWEEN THREE ONE AND SIX HUNTRED TO THREE ONE AH SIX AND TWO NOT THIS MATTER EXPECTS DO LEMENG NOW OR E SO YOU HOW TO PUT ON ERISNO ORE HERE I'M IN DEATHS THE SIGNATURE OF THE MATTER YOU SEE SIMARERT IS ONE WHICH MEANS THE DOCUMENTS ARE VERY SIMILAR AND IFFUSE FINE E COSIN DISTANCE OF THE SAME THING IS GOING TOBE ZILO AH COSIN DISTANCE SO THIS IS STUDEMES LOKE SEE ONDRE EDITH TO MINOR SIXTEEN THIS IS VERY CLOSE TO ZEDO O KI NOW IF YOU ARE FIND OUT A DISTANCE BETWEEN LESSET THESE TWO DOCUMENTS YOU NO THREE ONE THREE TWO SIDAR I'VE ON THREE TIMES TIGALEKY ONE TIME I FON THREE DAM GALEKY TO TAMS STILL THEY BOTH ARE APOLOCUMENTS SO THAT IS AN POIN NINE SIX PO OBSIMILARITY ONE SIMILARDIMENTS THERE VERY VERY SIMILAR NO LESS LOOKAT SOME REAL DOCUMENTS I AM GOING TO TO CREATE SOME VALIABLE VALIABLES HERE WITH FINANCIAL OCUMAN STRING SHO CAN SEE THE FIRST TON HIS EYE FORN APOLOCUMAN APOLOCUMEN SAMSON SAMSON OQUE AND WHAT YOU CAN DO IS YOU CAN CREATE A BAND AS DID OF HIN SO'M GWING TIN PORPAND AS IER AND WE CREATED PAANASDIDA FRIN AND THE DIDA INTHE DIDA FEN WERE GOING TO HARDGOURD SOME OR AS BO KASE OF WHAT ALE THOSE RECOS AN THE FIRST DOCUMENT YOU SEE I DON'T GIT ONE TO THREE DIME SELEXIS ONE TAME OR GET O IPHORN THREE DAS AND GAALEXEY ONE DAY SIMILARLY I HAVE PRECOUNTED THE IPHON AND GALEXI VORCOUNT IN REMAINING DOCUMENTS AND THAT'S TAT'S WHAT IT LOOKS LIKE NOW IN THA INDECKS I WANT TO COPPLI THE DOCUMENT AS AN INDEX SO INSTEAD OF ZERO ONE TWO THREE I WILL SAY O GA DOCK ONE DOCK TWO LOC THREE AND MY DET OF HIM WILL LOOK LIKE THIS SO IT SAYS DOCUMEN ONE HAS I FON THREE TIMES GELEXY VONTIMES AND SO ONE YOU CAN CLEARLY SEE POSTER DOCUMENTS ARE APPLE DOCUMENT AND SECOND AND THIRD DOCUMENTS IRE SAMPSON DOCUMENT NOW WHEN YOU DO SOMETHING LIKE THIS IT RETURNS YOU FIRST DOCUMENT BUT IT IS TO DIMISSIONAL ARRAY ALREADY LEF HIM AND WAR COSE ANY SIMILARTY FUNCTION EXPECTS THAT DET OF HIM AND THAT'S FIVE WE ARE GOOD DOING THIS YOU KNOW ATHERWISE I WOULD HAVE DONE SIMPLY THIS I WANT A COMPELLE I SAY AR LOCUMENT ONE AND OCUMENT TWO SO I WOULD HAVE DONE THIS GREAT BUT SEE IT IT EXPECTS TOO DEMINSONARY AND TAT THOONLY REASON I AM DOING THIS KIND OF RANGE OU K GERE GETTING SOME ER LATI OH YAD THIS IS ASINDEXATERACTUALLY IT HAS TO RED THIS WAY SEE THEARE VERY SIMILLAR BOYN NAME FOR S AGAINST THE BOTAR APPLE DOCMENTS BUT IF YOU DO THE SAME THING WITH LISIDOCUMEN ONE AND OCUMEN TREE THE ARE NOT THAT SIMILAR SIX POINT SIX WHY POINT SIX WELL LET'S COMPEREDOCK ONE AND DOCTESEE THREE ONE AND ONE THREE THIS SHUM SIMILARITY YOU KNOW BUT UH IF YOU DO LOG THREE AND FOUR LETES THEN THEY WULLD BE MORE CLOSER TO ONE SEEPORN NINE EG AND NOW FOR THE SHEAME A DOCUMENTS IF I DO CLOIN DISTANCE IT WILL BE POIN ZIROON SO COSSIN DISTANCE IS ONE MINOR SIMILARITY SO IF WE ADD THESE TWO NUMBER THE VELL I UP TO ONE THAT'S ALL PRETTY MUCH I HAD IF YOU WON'T TO DO THE SAME THING IN DENSER FLOW THAN DENSERFLOW HAS IN THIS PARTICULAR FUNCTION THAT YOU CAN USE I HOPE YOU WULD LIKE THIS REDOU IF YOU DID PLEASE SHAREY WITH YOUR FRIENDS THANK YOU\n"
          ]
        }
      ],
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline\n",
        "\n",
        "# Load your fine-tuned model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Create the ASR pipeline with chunking enabled, explicitly providing feature_extractor and tokenizer\n",
        "asr_pipeline = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    feature_extractor=processor.feature_extractor,  # Use the feature extractor\n",
        "    tokenizer=processor.tokenizer,  # Use the tokenizer\n",
        "    chunk_length_s=3  # Set chunk length in seconds (e.g., 5 seconds)\n",
        ")\n",
        "\n",
        "# Load the audio file path\n",
        "audio_path = \"/content/sample_data/audios/audio_1.mp3\"\n",
        "\n",
        "# Perform inference with chunking and streaming\n",
        "transcription = asr_pipeline(audio_path)\n",
        "\n",
        "# Print the final transcription\n",
        "print(\"Transcription:\", transcription['text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using the model as it is but this time chunking the audio signal."
      ],
      "metadata": {
        "id": "sr_rO-J3y5g0"
      },
      "id": "sr_rO-J3y5g0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2-hsO9jKMegU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-hsO9jKMegU",
        "outputId": "ac216a02-6d44-4771-e44a-6044988b3af7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription: AVAR CAT TEACHER IS GOING TO TEACH YOU KOSINSIMILARTY AND COSAN DISTANCE HOW IT IS USING DITTA SCIENCE WILL LOOK AT SOMTYRIAN THEN WE'LL MOON TO PIT AND COR TE SEE YOUR DITTA IENT IS WORKING FOR SOME FINANCIAL COMPANY WHERE ON A GUGALL DRIVE YOU HAVE BUNCH OF FINANCIAL DOCUMENTS NOW YOU DON'T KNOW FOR WITH WHAT IS THE COMPOLY ASSOCIATED WITH EACH OF THESE DOCUMENTS BUT WHEN YOU OPEN THE DOCUMENT WHEN YOU READ IT YOU CAN KIND OF FIGURE OUT THAT THIS IS PROBABLY ABOUT APPLE WHY BECAUSE IPONIS MENSIONED SO MANY TIMES SO WHEN YOU ARE READING ABOUT APPLE FINANCIAL REPORT THEY MIGHT MENTION GALECKSY AS WELL BECAUSEHAT THER MAIN COMPETITOR BUT HERE THE RICIO OF IPHON TO GELEXES IPHON HAVE TIPHON IS MENTIONED THREE TIMES GELXIS MENTIONED ONE TIME SO YOU KNOW AL YOU SEE IPHONACURRONCS MORE MUCH MORE THAN GELEXY IF YOUR SAMSON DOCUMENT OF COURSE SAMSON COME NEW DOCUMENT WILL HAVE MORE MENTION OF GELEXYTHEN IPHONE NOW LET'S SEE NEW DOCUMENT COMES IN AND YOU DON'T KNOW WHAT IS THE COMMNY ASSOCIATED WITH IT HERE YOU CAN AGAIN COUNT IPHONE ANGALEXEY AND YOU FIND THAT IPHONI SIX TIMES MENTION IN THE DOCUMENT GALEXY'S MENTIONED TWO TIMES LOOKING AT THE SITUATION AS I DID ASENTIS YOU CAN FIGURE OT A SYMPLE FORMULA THAT VENEVER A RASHO OF IPONDO GALAXY IS THREE TO ONE IT SHOULD BE AN APPL DOCUMENT SO NOW YOU CAN OUTO ANNOTATE APPLE AS A COMPANY THIS IS A COMMON PROBLEM IN FINANCIAL INSTITUTE WHERE YOU HAVE A DOCUMN AN YOU WONE TOO TAG IS A MATA DATA TO IT AND WE CAN USE SOME AUTOMATION SOME RULE SOME COODING TO OTTO ENNODATE YOU KNOW YOU CAN MANUALLY ANNOTATE IT BUT USING THIS PARTICL A FORMULA YOU CAN ATTOMATE THE NOTATION PRUCESS UNFORTUNATELY THINGS IN REAL LIFEAR QUITE DIFFERENT IN OUR DOCUMENT YOU KNOW THERE MIGHT BE MENTIONED OF I PER GUGOL PICKEL IS ANOTHER COMPETITOR SO NOW HOW DO YOU COME UP WITH YOUR FORMULA MAYBE YOU SAY O KE IPHON TO GELEXIRI SOU HAS TO BE THREE TO ONE AND IPON TO PICELRESHO HAS TO BE THREE TO ONE AND I PARE TO PICTURE RESSOU HAS TO BE TOOTO ON WELL THAT'S TOO COMPLICATED MY BABY GOT CONFUSED WELL WECTOR MATHEMATICS COMES AT RISCU THIS THING CAN BE PRESENTED AS A WECTOR HERE AND USING VECTOR MATHEMATICS WE CAN FIGURE OUR DOCUMENT SIMILARITY LET ME GO BACK TO THE SIMPLE CASEHAT WE SAW BEFORE HOW DO YOU REPRISON THIS AS A WACTOR WON IXSECCIS I AVIFE ON WORGOUNT WYAXIS AS GELAXY WORK WORGOWN SO YOU SEE THREE TO ONE AND THIS IS THE WACTOR WECTOR HAS MAGNITUDE AND DIRECTION BOTH AND WHEN I HAVE MY YELLOW DOCUMENT I HAVE ONLY SIX GALAXISTOO NOW LOOK AT THE ANGLE BETWEEN THESE TWOERROS BLUE AND YELLOW THE ANGLE IS ZERO SO THE ANGLE DETERMINES THE DOCUMENT SIMILARILY IF THE ANGLE IS ZU IT MEANS DOCUMENTS ARE VERY SIMILAR YOU MIGHT HAVE ANOTHER GREEN DOCUMENT YOU LIFE ON FIRE GALEXY ONE TIMEHERE THE ANGLE IS STILL NOT THAT HIGH AND ITS A LITTLE BIT OF AN ANGLE SO E CAN SAY THESE DOCUMENTS ARE STILL SIMILAR BUT WHEN YOU HAVE A SAMSON DOCUMENT WHERE IFTON IS MENTION ONLY ONCE GELEXIS FOR YOU SEE THE ANGLE IS MUCH BIGGER SO WHEN YOU HAVE A BIGGER ANGLE YOU CAN SAY THESE DOCUMENTS ARE NOT SIMILAR MEANINGIF FOR YELLOWELL ARROW I NO COMPANY'S APPLE FOR BLUE ARROW II CAN SAY COMPRIES DEF DEFINITELY NOT APPLE WORSE IS WERAVE A GREEN ARROW WHERE THE TETAR TO ANGLE IS MUTH YOU OW MUCH CLOSER TO YELLOW ARROW YOU CAN SAE GREEN AND YELLOW DOCUMENTS ARE SIMILAR WERE AS BLUE ASNOT SIMILAR LET'S SAY THE ANGLE BETWEEN THESE TWO SEVENTEEN DEGREE YOU CAN USE THIS ANGLE TO DEFINE DOCUMENT SIMILARITY ONE THING YOU CAN SAY IS DOCUMENT SIMILARITY THIS IS SEVENTEEN DEGREE HU I MEAN GOING TI SAY THAT IT DOESN'T SOUNN THAT OBVIOUS OR INDUTIVE IT'S NOT A GOOD WAY TO REPRESENT SIMILARITY WHATIF I CAN PRESENT THE SIMILARITY BETWEEN SOME NUMBER BETWEEN LESSEZIRO DOAN WHEN I SAI DOCUMENT SIMILAR TIS POINT NINE WHICH MEANS THERE NINETY PERSON SIMILAR LY NOW IF THE DOCUMENT SIMILARITIESONE THEY'RE VERY SIMILAR IF ITIS ZERO THEY'RE VERY DIFFERENT OKE SO HOW CAN YOU TRANSFORM THIS DIGRE ANGLE INTO ARANGE BECAUSE THAT RAIN SEEMS LIKE A GOOD CONVENTION YOU JUST TAKE KOSIN IF YOU DON'T NOBER SIGN COSIN I MADE A PREVIVEVIO PREVIOUSLY IN THE SAME SERIOUS WATCH THAT BUT KOSIN OF SEVENTEEN IS POY NINETY FIVE NOW SEE MY CONVENTION IS MUCH MORE OBVIOUS MUCH MORE EASIER I CAN SAY DOCUMENT SIMILARITY HERE BETWEEN GREEN AND YELLOW VECTOR IS POINT NINETY FIVE HOORAY ALL RIGHTNOW COSA AND SIMILARITIES NOTHING BUT A COSIGN OF AN ANGLE BETWEEN THE TWO WECTORS AND THE GREAT PART ABOUT MATH IS WE LOOKED TAT VERY SIMPLE SINARIO OF TO DIMENSION WECTOR IN REAL LIFE YOU WILL HAVE HUNDRED DIMENSION WECTR AND THESE DIMENSIONARBYSICALLY THE FEATURES SO YOUR DOCUMENT MIGHT HAVE HUNDRED FEATURES AND YOU CAN PRISON THOSE AS A VECTOR AND YOU CAN STILL DO THE MATS I THAT'S THE BEAUTY OF MAN YOU CANNOT VISULIZE HUNDRER DIMENSION BUT THE MAT WILL CONTINUE TO WAR OGAY THOUG ACADEMIC FORMULA ISBETWEEN E AND B DESERTTED TWO RECTORS AND THE COSIN SIMILARLY BETWEEN THESE TWO RECTORS IS DARD PRODUCT WHICH IS AD OR B DIVIED BY MAGNITROFE MAGNITROF B AN E DART B IS USUALLY BE COSTITASO YOU KNOW IT IS COSIN SIMILARLY IS NOTHING BUTAT COSTIDA ALL RIGHT NOW IF YOU HAVE ARROWS POINTING IN A SAME DIRECTION THEN THE COSIN SIMILARITY IS ONE WHICH MEANS THOSE ARROWS THOSE VECTORS THOSE DOCUMENTS ARE QUITE SIMILAR IF THEY ARE AT NINETY DEGREE MEANS TO SIMILARDYZERO THEYARE VERY DIFFERENT AND IF IT IS ONELIDI DIGREE SIMILARITIS MINE AS ONE WHICH MEANS THE REPRESENT OPPOSITE CONA CONCEPT NOW COSINE DISTANCE IS A VERY SIMPLE CONCEPT JUST IT T IS USED TO REPRESENT THE SAME THING IT'S ONE MINORS COSIN SIMILARITY SO HERE WHEN TWO ARROWS ARE POINTING IN THE SAME DIRECTTION WHEN YOU'RE TALKING ABOUT SEEM RECTOR THE COSAN DISTANCE WILL BE ZIRO SO WHEN YOU SAID DISTANCE IS ZERO WHICH MEANS THEY ARE SIMILAR THEY ARE CLOSER THAT'S THE ONLY IDEA BEHIND COSAN DISTANCE WHEN YOU HOR TWO ECTORS AT NINETY DEGREE COSAN DISTANCE IS ONE WHICH MEANS THERE VERY DIFFERENT AND COSAN DISTANCES OR REPRESENTED ONLY IN A POSITIVE SPACE THIS WHERE AND I'M NOT TALKING OVER THAT ONE EDYDYGRI CASE THAT WE SAW IN A PREVIOUS LIGHT LESS WRITE SOM PATAN GORNA WE'LL BE USING PITONS AS KILLON MOR DILL FOUR IN PORTIN COS AND SIMILARLY MATTER HERE AND WHEN YOU HAV THIS MATTER IT EXPECTS TWO ACTORS ANDTHOSE TWO ACTORS ARE GOING TO BE THE CASE THAT WE LOOKD TAT FOR APODOCUMEN WAS THREE ONE AND SIX AND TWO SO LET'S SEE WHAT IS A SIMILARITY BETWEEN THREE ONE AND SIX AND T THREE ONE AH SIX AND TWO NOT THIS MATHER EXPECTS TWO DEMATINAL OR A SO YOU HOW TOPUT ON ERISNOARAY HERE I'MIN THAT'S THE SIGNATURE OF THE MATTER YOU SEE SIMELARTYIS ONE WHICH MEANS THA DOCUMENTS ARE VERY SIMILAR AND IF YOU FINE A COSIN DISTANCE OF THE SAME THING IS GOING  BE ZEO AH COSIN DESTON SO THIS IS STUDEMAGE OK SEE ONE RE EDITH TO MINOR SIXTEEN THIS IS VERY CLOSE TO ZELO O GA NOW IF YOU ALL FIND OUT A DISTANCE BETWEEN LISTAT THESE TWO DOCUMENTS YOU NONL THREE ONE THREE TWOI THE AR I FON THREE TIMES TOGELEXSY ONE TIMES I FON THREETEM GELEXYTWO TIMES STILL THEY BOTH AR APODOCUMENTS SO THAT IS AN POIN NINE SIX PO O SIMILARITY ONE SIMILAR DYMEANS THERE VERY VERY SIMILAR NOW LESS LOOK AT SOME REAL DOCUMENTS I AM GOING TOTO CREAT SOME GEABLE VALUABLES HERE WITH FIN INCIAL LOCUMAN STRING SHE CAN SEE THE FIRST ON HIS EYE FON APOLOCUMAN APOLOCUMAN SAMSON SAMSON OKE AND WHAT HE CAN DO IS YOU CAN CREATE A PANDAS DID OF HIM SAMSWINBIN PORPAND ASER AND WECREATED PANDAS DIDA FREM AND THE DIDE IN THE DID AF HIM BE GOING TO HARGORD SOME ARAYS PO CASE OF WHAT ARE THOSE RECORS IN THE FIRST DOCUMENT YOU SEE I FON'T GEM ONE TO THREE TIMES GALEXIS ONE TIME OR GES IPHON THREE TIMES GALEY ONE TIME SIMILARLY I HAVE PRECOUNTED THE IPHON AN GALEXI VORCOUNT IN REMAINING DOCUMENTS AND TEATHS TAT'S WHAT IT LOOKS LIKE NOW IN THE INDEX I WANT TO TO APPLY THE DOCUMENT AS AN INDEX SOINSTEAD OF ZEROON TO TREE I WILL SAY O A DOC ONE DOC TO DOCTREE AND MY DEAT OF HIM WILL LOOK LIKE THIS SO IT SAYS DOCGOMEN ONE HAS I FON THREE TIMES GELEXY VONE TIMES AND SO ONE YOU CAN CLEARLY SEE FORST TO DOCUMENTS ARE APPLE DOCUMENTS AND SECOND AND THIRD DOCUMENTS IRE SAMPSON DOCUMENNOW WHEN YOU DO SOMETHING LIKE THIS IT RETURNS YOU FORCE DOCUMENT BUT IT IS TO DIMINSIONAL ARRAY ALREADYT OF HIM AND OAR COS AN SIMILARLY FUNCTION EXPECTS THAT DEDOF HIMAN THAT'S FIVE WE ARE GOOD DOING THIS YOU KNOW ALTHOUGH AS I WOULD HAVE DONE SIMPLY THESI WANT TO COMPELLED A SAY A LOCUMENT ONE AND DOCUMENT TOO SO I WOULD HAVE DONE THIS REAT BUTSE IT IT EXPECTS TOO DOMENSIONARY AND THAT'S THE ONLY REASON I AM DOING THIS KIND OF RANGEO K YEARE GETTING SOME ER LESSE OH YAD THIS IS AS INDEXERAACTUALLY IT HAS TO BE THIS WAY SEE THERE VERY SIMILLA BOI NANFOR SO YE GAINS TE BOTAR APPL DOCUMENTS BUT IF YOU DO THE SAME THING WITH LESS A DOCUMEN ONE AND DOCUMEN TREE THE ARE NOT THAT SIMILAR TE POINTS SIX WHY POINT SIX WELL LET'S COMPARE DOC ONE AND DOCTO SEE THREE ONE AND ONE THREE THIS SOME SIMILARITY YOU KNOWBUT AH IF YOU DO LOK THREE AND FOUR LESSY THEN THER'LL BE MORE CLOSER TO ONE TEAPONT NINETY EIGHTAND NOW FOR THE SAME A DOCUMENTS IF I DO COSIN DISTANCE IT WILL BE POINTS IT ON SO COSINE DISTANCE IS ONE MINORS SIMILARITY SO IF YOU ADD THESE TWO NUMBERS THEY WILL AD UP TO ONE THAT'S ALL PRETTY MUCH I HAD IF YOU WANT TO DO THE SAME THING IN DENSER FLOR THANENCERFLAW HAS THIS PARTICULAR FUNCTION THAT YOU CAN USE I HOPE YOU WILL LIKE THIS WILL YOU IF YOU DID PLEASE OSHAR IT WITH YOUR FRIENDS THANK YOU\n"
          ]
        }
      ],
      "source": [
        "# Chunking and streaming audio:\n",
        "\n",
        "import librosa\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "\n",
        "# Load pretrained model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Load audio and define chunk size (1-second chunks)\n",
        "audio_path = \"/content/sample_data/audios/audio_1.mp3\"\n",
        "audio_input, sample_rate = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "# Define chunk size (1 second of audio in 16000 Hz is 16000 samples)\n",
        "chunk_size = 320000  # 1 second\n",
        "\n",
        "# Process audio in chunks\n",
        "transcription = \"\"\n",
        "for i in range(0, len(audio_input), chunk_size):\n",
        "    audio_chunk = audio_input[i:i+chunk_size]\n",
        "\n",
        "    # Preprocess the chunk\n",
        "    inputs = processor(audio_chunk, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Predict logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs.input_values).logits\n",
        "\n",
        "    # Get predicted ids\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Decode the predictions\n",
        "    transcription_chunk = processor.decode(predicted_ids[0])\n",
        "\n",
        "    # Append the chunk's transcription to the full transcription\n",
        "    transcription += transcription_chunk\n",
        "\n",
        "print(\"Transcription:\", transcription)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In the below code, we are doing chunking, and multi processing with the raw model. We also divide the into batch and process them."
      ],
      "metadata": {
        "id": "OUD-okmFzBol"
      },
      "id": "OUD-okmFzBol"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nnZ_XqrPQCIC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnZ_XqrPQCIC",
        "outputId": "f250271d-979f-431a-d31c-9f75ff437e74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription: AVAR CAT TEACHER IS GOING TO TEACH YOU COSINSIMILARITY AND COSAN DISTANCE HOW IT IS USIN DETA SINSE WELL LOOK AT SOME THYRIAN THEN WE'LL MOON TO PIT AND COR THEY SAE YOUR LETTES IENT IS WORKING FOR SOME FINANCIAL COMPANY WHERE ON A GUGALL DRIVE YOU HAVE BUNCH OF FINANCIAL DOCUMENTS NOW YOU DON'T KNOW FOR WI WHAT IS THE COMPANY ASSOCIATED WITH EACH OF THESE DOCUMENTS BUT WHEN YOU OPEN THE DOCUMENT WHEN YOU REAL IT YOU CAN KIND OF FIGURE OUT THAT THIS IS PROBABLY ABOUT APPLE WHY BECAUSE IPHON IS MENTIONED SO MANY TIMES SO WHEN YOU'RE READING ABOUT APPLE FINANCIAL REPORT THEY MIGHT MENTION GELEXY AS WELL BECAUSE THAT'S THEIR MAIN COMPETITOR BUT HERE THE RISIO OF IPHON TO GELEXEES IPHON HAV TR IPHON IS MENTIONED THREE TIMES GELXIS MENTIONED ONE TIME SO YOU KNOW ALL YOU SEE IPHONA CURRUNCS MORE MUCH MORE THAN GALAXEY IF YOU ARE SAM AND DOCUMENT OF COURSE SAMSON COM NE DOCUMENT WILL HAVE MORE MENTION OF GALEXY THAN IPHON NOW LET'S SEE NEW DOCUMENT COMES IN AND YOU DON'T KNOW WHAT IS THE COMMNY ASSOCIATED WITH IT HERE YOU CAN AGAIN COUNT IPHONE ANGALEXY AND YOU FIND THAT IPHONI SIX TIMES MENTION IN THE DOCUMENT GALEXY'S MENTIONED TWO TIMES LOOKING AT THE SITUATION AS I DID AS I INTEST YOU CAN FIGURE DO R A SIMPLE FORMULA THAT VENEVER A RISO OF IPONDO GALAXY IS THREE TO ONE IT SHOULD BE AN APAL DOCUMENT SO NOW YOU CAN OUTO ANNOTATE APPLE AS A COMPANY THIS IS A COMMON PROBLEM IN FINANCIA INSTITUTE WHERE YOU HAVE A DOCUMENT AND YOU WONT TO TAG IS A MATTAR DETTA TO IT AND WE CAN USE SOME AUTOMATION SOME RULE SOME COODING TO OTTO ANODIT YOU KNOW YOU CAN MANUALY ANNOTATED BUT USING THIS PARTICL A FORMULA YOU CAN AUTOMATE THE NOTATION PRUSES UNFORTUNATELY THINGS IN REAL LIFE ARE QUITE DIFFERENT IN OUR DOCUMENT YOU KNOW THERE MIGHT BE MENTIONED OF I PED GOGOLPICXEL IS ANOTHER COMPETITOR SO NOW HOW DO YOU COME UP WITH YOUR FORMULA MAYBE YOU SAY OCE IPHON TO GELLEXERIES SO HAS TO BE THREE TO ONE AND IPHON TO PIXOLRISHO HAS TO BE THREETO ONE A I PARE TO PICTURE RESTUR HAS TO BE TO TO ONE WELL THAT'S TOO COMPLICATED MY BABY GOD CONFUSED WELL WECTOR MATHEMATICS COMES AT RISCUE THIS THING CAN BE PRESENTED AS A WEACTOR HERE AND USING VECTOR MATHEMATICS WE CAN FIGURE OUR DOCUMENT SIMILARITY LET ME GO BACK TO THE SIMPLE CASE THAT WE SAW BEFORE HOW DO YOU REPRESENT THIS AS A WECTOR WHEN EXECIS I HAVEIFE ON WORCOUNT WAXES AS GALOXY WORK WORCOWN SO YOU SEE THREE TO ONE AND THIS IS THE WACTOR WACTOR HAS MAGNITUDE AND DIRECTION BOTH AND WHEN IHAVE MY YELLOW DOCUMENT I HAVE ONLY SIX GALEXES TOO NOW LOOK AT THE ANGLE BETWEEN THESE TWOARRORS BLUE AND YELLOW THE ANGLE IS ZERO SO THE ANGLE DETERMINES THE DOCUMENT SIMILARITY IF THE ANGLE IS ZU IT MEANS DOCUMENTS ARE VERY SIMILAR YOU MIGHT HAVE ANOTHER GREEN DOCUMENT YIU LIFE ON FIE GELEY ONE TIME HERE THE ANGLE IS STILL NOT THAT HIGH AND ITS A LITTLE BIT OF AN ANGLE SO E CAN SAT THESE DOCUMENTS ARE STILL SIMILAR BUT WHEN YOU HAVE A SAMSON DOCUMENT WHERE IPON IS MENTION ONLY ONCE GELEXES FOR YOU SEE THE ANGLE IS MUCH BIGGER SO WHEN YOU HAVE A BIGGER ANGLE YOU CAN SAY THESE DOCUMENTS ARE NOT SIMILAR MEANING IF FOR YELLWELL ARROW I NO COMPANY'S APPLE FOR BLUE ARROW II CAN SAY COMPLIES DEF DEFINITELY NOT APPLE WORS IS WER HAVE A GREEN ARROW WHERE THE TITAR TO ANGLE IS MUTH YOU OW MUCH CLOSER TO YELLOW ARROW YOU CAN SAY GREEN AND YELLOW DOCUMENTS ARE SIMILAR WHEREAS BLUE IS NOT SIMILAR LET SAY THE ANGLE BETWEEN THESE TWO I SEVENTEEN DEGREE YOU CAN USE THIS ANGLE TO T FINE DOCUMENTAT SIMILARITY ONE THING YOU CAN SAY IS DOCUMENT SIMILARITY THIS I SEVENTEEN DEGREE HUM I MEAN WEN I SAY THAT IT DOESN'T SOUND THAT OBVIOUS OR INTUTIVE IT'S NOT A GOOD WAY TO REPRESENT SIMILARITY WHAT IF I CAN PRISON THE SIMILARITY BETWEEN SOME NUMBER BETWEEN LESSEZI RODOAN WHEN I SA DOCUMEN SIMILARDISPOINT NINE WHICH MEANS THEY'RE NINETY PERSON SIMILAR LOU KNOW IF THE DOCUMENT SIMILARTIS ONE THEY'RE A VERY SIMILAR I IT IS ZIRO THEY'RE VERY DIFFERENT O K SO HOW CAN YOU TRANSFORM THIS DIGRE ANGLE INTO ARANGE BECAUSE THAT REIN SEEMS LIKE A GOOD CONVENTION YOU JUST TAK COSIN IF YOU DON'T NOBER SIGN COSIN I MADE PREVIVIDIO PREVIOUSLY IN THE SAME SERIOUS WATCH THAT BUT KOSIN OF SEVENTEEN IS POIN NINETY FIVE NOW SEE MY CONVENTION IS MUCH MORE OBVIOUS MUCH MORE EASIER I CAN SAY DOCUMENT SIMILARITY HERE BETWEEN GREEN AND YELLOW VECTOR IS POINT NINETY FIVE HOORAY ALL RIGHT NOW COSIN SIMILARITY IS NOTHING BUT A COSIGN OF AN ANGLE BETWEEN THE TWO ACTORS AND THE GREAT PART ABOUT MAT IS WEL G THAT VERY SIMPLE SINARIO OF TO DIMENSION VECTOR IN REAL LIFE YOU WILL HAVE A HUNDRED DIMENSION VECTOR AND THESE DIMENSIONS ARE BYSICALLY THE FEATURES SO YOUR DOCUMENT MIGHT HAVE HUNDRED FEATURES AND YOU CAN PRISON THOSE AS A VECTOR AND YOU CAN STILL DO THE MAS I THAT'S THE BEAUTY OF MAT YOU CANNOT VISULIZE HUNDRED DIMENSION BUT THE MAT WILL CONTINUE TO WAR OGEY THOUGH ACADEMIC FORMULA IS BETWEEN E AND B DESERTHE TWO RECTORS AND THE COSIN SIMILARLY BETWEEN THESE TWO RECTORS IS LOD PRODUCT WHICH IS ADOR B DIVIDE BY MAGNITROPE MAGNETROFB AN E DART B IS USUALLY BE COS TITASO YOU KNOW IT IS COSIN SIMILARLY IS NOTHING BUT COSTITA ALL RIGHT NOW IF YOU HAVE ARROWS POINTING IN A SAME DIRECTION THEN THE COSIN SIMILARITY IS ONE WHICH MEANS THOSE ARROWS THOSE VECTORS THOSE DOCUMENTS ARE QUITE SIMILAR IF THEY ARE AT NINETY DEGREE MEANS TO SIMILARDIZERO THEY ARE VERY DIFFERENT AND IF IT IS ONY DY DEGREE SIMILAR DISMINOUS ONE WHICH MEANS THE REPRESENT OPPOSITE CONA CONCEPT NOW COSIN DISTANCE IS OF VERY SIMPLE CONCEPT G UST IT IT IS USED TO REPRESENT THE SAME THING IT'S ONE MINAS COSIN SIMILARITY SO HERE WHEN TWO ARROWS ARE POINTING IN THE SAME DIRECTION WHEN YOU'RE TALKING ABOUT SAME RECTOR THE COSAN DISTANCE WILL BE ZERO SO WHEN YOU SAID DISTANCE IS ZERO WHICH MEANS THEY ARE SIMILAR THEY ARE CLOSER THAT'S THE ONLY IDEA BEHIND COSAN DISTANCE WHEN YOU HAR TWO ACTORS AT NINETY DEGREE COSAN DISTANCE IS ONE WHICH MEANS THERE VERY DIFFERENT AND COSAN DIS STANCES AR REPRESENTED ONLY IN A POSITIVE SPACE THIS WHELE I'M NOT TALKING OVER THAT ONE EDYDIGRI CASE THAT WE SAW IN A PREVIOUS LIHT LASS WRITE SOM PATAM GONA WE'LL BE USING PITONS AS KILLON MOR DILL FOR INPORTING COS AND SIMILARLY MATTER HERE AND WHEN YOU HAW THIS MATTER IT EXPECTS TWO ACTORS AND THOSE TWO ACTORS ARE GOING TO BE THE CASE THAT WE LOOKED AT FOR APODOCUMEN WAS THREE ONE AND SIX AND TWO SO LET'S SEE WHAT IS THE SIMILARTY BETWEEN THREE ONE AND SIX AND TWO TO THREE ONE AH SIX AND TWO NOT THIS MATTER EXPECTS DO DEMANGENAL OR A SO YOU HOW TO PUT ON ERISNO OREY HERE I'M THAT'S THE SIGNATURE OF THE MATTER YOU SEE SIMILARLY IS ONE WHICH MEANS THAT DOCUMENTS ARE VERY SIMILAR AND IF YOU FINE A COSINE DISTANCE OF THE SAME THING IS GOING O WILL BE E ZELO A COS IN DISTANCE SO THIS IS STUDE MASLOK SEE ONE REDITH TO MINOR SIXTEEN THIS IS VERY CLOSE TO ZELO O K NOW IF YOU A FIND OUT A DISTANCE BETWEEN LISTAT THESE TWO DOCUMENTS YOU NONL THREE ONE THREE TWO SEE THE AR I'VE ONE THREE TIMES TOGETH XE ONE TIMES I FON THREETAM GELECYTOTEMS STILL THEY BOTH ARE APOLOCUMENTS SO THAT IS AN POIN NINE SIX PO OR SIMILARITY ONE SIMILARDYMEANS THERE VERY VERY SIMILAR NOW LESS LOOK AT SOME REAL DOCUMENTS I AM GOING TOTO CREATE SOME VALIABLE VELIABLES HERE WIT FIN INCIAL DOCUMEN STRING SHE CAN SEE THE FOST ON HIS EYFN APOLOCUMEN APOLOCUMEN SAMSON SAMSON OKE AND WHAT HE CAN DO IS YOU CAN CREATE A PANDAS DID OF HIM SO'M GWINBIN PORPAND AS YEAR AND WE CREATE PANDAS DIDAFRIM AND THE DIDA IN THE DID AF HIM BE GOING TO HARGORD SOME ARAYS O KASE OF WHAT AR THOSE RECOS IN THE FIRST DOCUMENT YOU SEE I TON'T KEM ONE TO THREE TIMES GE ALEXIS ONE TIME OGAS IPHON THREE TIMES G LAXY ONE TIME SIMILARLY I HAVE PRCOUNTED THE IPHON AND GALEXY VORCOUNT IN REMAINING DOCUMENTS AND DEATHS THAT'S WHAT IT LOOKS LIKE NOW IN THE INDEX I WANT TO TO APPLY THE DOCUMENT AS AN INDEX SO INSTEAD OF ZERO WAN TUTRE I WILL SAY OF GA DOCK ONE DOCTO DOCTREY AND MY DEAD OF HIM WILL LOOK LIKE THIS SO IT SAYS DOCTUMEN ONE HAS I FON THREE TIMES GELEXI VONE TIMES AND SO ONE YOU CAN CLEARLY SEE FORST TO DOCUMENTS A APLE DOCUMENTS AND SECOND AND THIRD DOCUMENTS ARE SAMPSON DOCUMENTS NOW WHEN YOU DO SOMETHING LIKE THIS IT RETURNS YOU FORST DOCUMENT BUT IT IS TO DEMINSNAL ARRAY ALREADIT OF HIM AND OR COS AND SIMILARLY FUNCTION EXPECTS THAT DID OF HIM AND THAT'S FIVE WE ARE GOOD DOING THIS YOU KNOW ALTHOUGH AS I WOULD HAVE DONE SIMPLY THIS I WANT A COMPELED I SAY A DOCUMENT ONE AND DOCUMENT TOO SO I WOULD HAVE DONE THIS RIGHT BUT SEE IT IT EXPECTS TOO DOMENSIONY AND THAT'S THE ONLY REASON IAM DOING THIS KIND OF RANGE O K YOUARE GETTING SOME ERRR LESSE OH YAD THIS IS IS INTEXERRACTUALLY IT HAS TO BE THIS WAY SEE THEYARE VERY SIMILAR POIN NAME FOR S WE GAINST THE BOTAR APPL DOCUMENTS BUT IF YOU DO THE SAME THING WITH LESS A DOCUMEN ONE AND DOCUMENT THREE THE A ERE NOT THAT SIMILAR TE POINT SIX WHY POINT SIX WELL LET'S COMPAREADOC ONE AND DOCTA SEE THREE ONE AND ONE THREE THI'S SOME SIMILARIY YOU KNOW BUT OH IF YOU DO DO THREE AND FOUR LESSIT THEN THEY'LL BE MORE CLOSER TO ONE SEE POIN INDEED AND NOW FOR THE SAME I DOCUMENTS IF I DO COSIN DISTANCE IT WILL BE POINTS IR ON SO COSAN DISTANCE IS ONE MINOR SIMILARITY SO IF YOU ADD THESE TWO NUMBERS THEY WILL AD UP TO ONE THAT'S ALL PRETTY MUCH I HAD IF YOU WANT TO DO THE SAME THING IN DENSER FLOW THAN DENSER FLOW HAS THIS PARTICULAR FUNCTIO N THAT YOU CAN USE I HOPE YOU WILL LIKE THIS READO IF YOU DID PLASE TOSHERIT WITH YOUR FRIENDS THANK YOU\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# Load pretrained model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Ensure the model runs on CPU\n",
        "model.to(\"cpu\")\n",
        "\n",
        "# Load audio and define chunk size (1-second chunks)\n",
        "audio_path = \"/content/sample_data/audios/audio_1.mp3\"\n",
        "audio_input, sample_rate = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "# Define chunk size (1 second of audio in 16000 Hz is 16000 samples)\n",
        "chunk_size = 440000  # 1 second\n",
        "batch_size = 2  # Number of chunks to process in a batch\n",
        "\n",
        "# Split the audio into chunks\n",
        "chunks = [audio_input[i:i+chunk_size] for i in range(0, len(audio_input), chunk_size)]\n",
        "\n",
        "# Function to process a batch of audio chunks\n",
        "def process_batch(batch):\n",
        "    inputs = processor(batch, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs.input_values).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Decode the batch of predicted ids\n",
        "    transcriptions = processor.batch_decode(predicted_ids)\n",
        "\n",
        "    return transcriptions\n",
        "\n",
        "# Function to handle multiprocessing for batches\n",
        "def parallel_processing(chunks, batch_size):\n",
        "    # Break the chunks into batches\n",
        "    batches = [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]\n",
        "\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        results = pool.map(process_batch, batches)\n",
        "\n",
        "    # Combine all the transcriptions\n",
        "    return \" \".join([transcription for batch in results for transcription in batch])\n",
        "\n",
        "# Execute the parallel processing\n",
        "transcription = parallel_processing(chunks, batch_size)\n",
        "print(\"Transcription:\", transcription)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In this approach we are using HuggingFace pipeline api to have a more structured. And, we are doing multiprocessing."
      ],
      "metadata": {
        "id": "ryO4u8gSzwzS"
      },
      "id": "ryO4u8gSzwzS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beSCuyv-00rN",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "beSCuyv-00rN",
        "outputId": "54f2451f-9d2e-46c4-de48-1014a74fe4da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline\n",
        "import multiprocessing as mp\n",
        "import librosa\n",
        "\n",
        "# Load your fine-tuned model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Create the ASR pipeline, explicitly providing feature_extractor and tokenizer\n",
        "asr_pipeline = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    feature_extractor=processor.feature_extractor,  # Use the feature extractor\n",
        "    tokenizer=processor.tokenizer  # Use the tokenizer\n",
        ")\n",
        "\n",
        "# Function to process the entire audio file using the ASR pipeline\n",
        "def process_audio(audio_input):\n",
        "    return asr_pipeline(audio_input)['text']\n",
        "\n",
        "# Multiprocessing handler\n",
        "def multiprocess_asr(audio_path, num_workers=mp.cpu_count()):\n",
        "    # Load the full audio file\n",
        "    audio_input, sample_rate = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Use multiprocessing to process the entire audio file in parallel\n",
        "    with mp.Pool(processes=num_workers) as pool:\n",
        "        transcriptions = pool.map(process_audio, [audio_input])\n",
        "\n",
        "    # Combine all transcriptions (though it's just one in this case)\n",
        "    final_transcription = \" \".join(transcriptions)\n",
        "    return final_transcription\n",
        "\n",
        "# Load the audio file path\n",
        "audio_path = \"/content/sample_data/audios/audio_1.mp3\"\n",
        "\n",
        "# Run the multiprocessing ASR inference\n",
        "transcription = multiprocess_asr(audio_path)\n",
        "\n",
        "# Print the final transcription\n",
        "print(\"Transcription:\", transcription)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wave2Vec2 Model without Multi Processing, Quantization or Chunking."
      ],
      "metadata": {
        "id": "w7SavCcwyV6t"
      },
      "id": "w7SavCcwyV6t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c53bd0-adcc-4a80-9357-30c7b8106e3f",
      "metadata": {
        "id": "01c53bd0-adcc-4a80-9357-30c7b8106e3f"
      },
      "outputs": [],
      "source": [
        "# from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "# import torch\n",
        "# import soundfile as sf\n",
        "# import librosa\n",
        "\n",
        "# # Load pretrained model and processor\n",
        "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# # Load your audio file (must be 16kHz)\n",
        "# audio_input, sample_rate = librosa.load(\"/content/sample_data/audios/audio_1.mp3\", sr=16000)\n",
        "\n",
        "# # Preprocess the audio and convert it to input format\n",
        "# inputs = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
        "# # device = torch.device(\"cpu\")\n",
        "# # model.to(device)\n",
        "\n",
        "# # Quantize the model\n",
        "# model_quantized = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# import torch\n",
        "# import os\n",
        "\n",
        "# def get_model_size(model, filename='model.pt'):\n",
        "#     torch.save(model.state_dict(), filename)\n",
        "#     return os.path.getsize(filename)   # Size in MB\n",
        "\n",
        "# original_size = get_model_size(model, 'original_model.pt')\n",
        "# quantized_size = get_model_size(model_quantized, 'quantized_model.pt')\n",
        "\n",
        "# print(f\"Original Model Size: {original_size:.2f} MB\")\n",
        "# print(f\"Quantized Model Size: {quantized_size:.2f} MB\")\n",
        "\n",
        "# # Get the logits (predictions) from the quantized model\n",
        "# with torch.no_grad():\n",
        "#     logits = model_quantized(inputs.input_values).logits\n",
        "\n",
        "# # Decode the predicted text\n",
        "# predicted_ids = torch.argmax(logits, dim=-1)\n",
        "# transcription = processor.decode(predicted_ids[0])\n",
        "\n",
        "# print(\"Transcription:\", transcription)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d60fce90",
      "metadata": {
        "id": "d60fce90"
      },
      "source": [
        "### Tensorflow Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b048a7",
      "metadata": {
        "id": "51b048a7"
      },
      "outputs": [],
      "source": [
        "from transformers import Speech2TextProcessor, TFSpeech2TextForConditionalGeneration\n",
        "\n",
        "\n",
        "processor_tf = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "model_tf = TFSpeech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba5266ba",
      "metadata": {
        "id": "ba5266ba",
        "outputId": "a958bc6d-dced-44e3-b538-b20b2d078ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8, 1024, 80)\n"
          ]
        }
      ],
      "source": [
        "max_length = 1024  # Define a max length for the input sequence\n",
        "\n",
        "input_features = processor_tf(\n",
        "    dataset1['train'][\"audio\"],\n",
        "    sampling_rate=16000,\n",
        "    return_tensors=\"tf\",\n",
        "    padding=True,  # Apply padding\n",
        "    max_length=max_length,  # Truncate to max_length\n",
        "    truncation=True  # Ensure truncation is applied\n",
        ")\n",
        "\n",
        "# Check input shape again\n",
        "print(input_features['input_features'].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec182dc",
      "metadata": {
        "id": "6ec182dc",
        "outputId": "c37a9d97-1fe3-4869-891f-e813bb0e36c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8, 1024)\n"
          ]
        }
      ],
      "source": [
        "max_length = 1024  # Define a max length for the input sequence\n",
        "\n",
        "input_labels = processor_tf.tokenizer(\n",
        "    dataset1['train'][\"transcript\"],\n",
        "#     sampling_rate=16000,\n",
        "    return_tensors=\"tf\",\n",
        "    padding=True,  # Apply padding\n",
        "    max_length=max_length,  # Truncate to max_length\n",
        "    truncation=True  # Ensure truncation is applied\n",
        ")\n",
        "\n",
        "# Check input shape again\n",
        "print(input_labels['input_ids'].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d115368-1e0c-4f30-98b2-7142f94ff6c6",
      "metadata": {
        "id": "9d115368-1e0c-4f30-98b2-7142f94ff6c6",
        "outputId": "1bbd9a4f-db68-46bf-98a6-8f6114454b46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int32, numpy=\n",
              "array([[ 263,   15, 3259, ...,    1,    1,    1],\n",
              "       [3774,   52,   60, ..., 1173,   36,    2],\n",
              "       [  12,   67,  451, ...,    3,   31,    2],\n",
              "       ...,\n",
              "       [  52,   60,  243, ..., 1671,   25,    2],\n",
              "       [  52,   67,  104, ...,    9,  453,    2],\n",
              "       [ 216, 1378, 2957, ...,    4, 4906,    2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int32, numpy=\n",
              "array([[1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a02e2c4",
      "metadata": {
        "id": "1a02e2c4"
      },
      "outputs": [],
      "source": [
        "padded_inputs= np.array(input_features['input_features'], dtype= np.float32)\n",
        "padded_labels = np.array(input_labels['input_ids'], dtype= np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20858563-9920-4b10-ab85-6a474439c542",
      "metadata": {
        "id": "20858563-9920-4b10-ab85-6a474439c542"
      },
      "outputs": [],
      "source": [
        "# padded_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "876bd0a1",
      "metadata": {
        "id": "876bd0a1"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# dataset = tf.data.Dataset.from_tensor_slices(encoded_audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f12ef13",
      "metadata": {
        "id": "1f12ef13",
        "outputId": "0fd363c1-f43f-4fcc-811f-cb5cee2f1e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8, 1024, 80)\n",
            "(8, 1024)\n"
          ]
        }
      ],
      "source": [
        "print(padded_inputs.shape)  # Should output (num_samples, sequence_length, num_features)\n",
        "print(padded_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "062e91dc-c258-4925-90e9-8e3e8a688d13",
      "metadata": {
        "id": "062e91dc-c258-4925-90e9-8e3e8a688d13"
      },
      "outputs": [],
      "source": [
        "import jiwer\n",
        "\n",
        "def wer_metric(y_true, y_pred):\n",
        "    # Convert the tensors to text sequences\n",
        "    y_true_text = processor_tf.batch_decode(y_true, skip_special_tokens=True)\n",
        "    y_pred_text = processor_tf.batch_decode(y_pred, skip_special_tokens=True)\n",
        "\n",
        "    # Calculate WER using jiwer\n",
        "    wer_score = jiwer.wer(y_true_text, y_pred_text)\n",
        "\n",
        "    return wer_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b3faf18",
      "metadata": {
        "id": "3b3faf18",
        "outputId": "9be4766f-9af4-4ead-f1c2-04cda8d8a6f3",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_8 (Conv1D)           (None, None, 32)          7712      \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, None, 64)          6208      \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPoolin  (None, None, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_70 (Dropout)        (None, None, 64)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d_4  (None, 64)                0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dropout_71 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 8)                 1032      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23272 (90.91 KB)\n",
            "Trainable params: 23272 (90.91 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into training and testing\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(padded_inputs, padded_labels, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "model_asr = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(None, 80)),  # Input shape: (sequence_length, features)\n",
        "\n",
        "    # Apply 1D convolutions over the input sequence\n",
        "    tf.keras.layers.Conv1D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
        "\n",
        "    # MaxPooling layer\n",
        "    tf.keras.layers.MaxPooling1D(),\n",
        "\n",
        "    # Dropout to prevent overfitting\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    # Use GlobalAveragePooling1D to remove the need for a fixed input length\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "\n",
        "    # Fully connected layers\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "    # Output layer (adjust number of units according to your task)\n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "model_asr.summary()\n",
        "\n",
        "# Define an optimizer and compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "# Compile the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7036eca-0e76-409e-bc6b-9e2edca13940",
      "metadata": {
        "id": "f7036eca-0e76-409e-bc6b-9e2edca13940",
        "outputId": "2383e81c-20fa-485f-e2c5-f087ef096df1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1024,)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ytrain[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a63ee98-ad41-4f6f-832f-94bb0e98a9c9",
      "metadata": {
        "id": "7a63ee98-ad41-4f6f-832f-94bb0e98a9c9",
        "outputId": "8336b7a3-45f4-485b-cd36-df8e53630a9b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 531, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_858/3856781796.py\", line 25, in <module>\n      model_asr.fit(xtrain, ytrain, epochs=3)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1081, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/losses.py\", line 2354, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/backend.py\", line 5762, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [6,8] and labels shape [6144]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_10082]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[43], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m model_asr\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy())\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmodel_asr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 531, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_858/3856781796.py\", line 25, in <module>\n      model_asr.fit(xtrain, ytrain, epochs=3)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1081, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/losses.py\", line 2354, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/backend.py\", line 5762, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [6,8] and labels shape [6144]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_10082]"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "\n",
        "labels= [1,2,3,1,2,3]\n",
        "\n",
        "# Define the custom loss function\n",
        "def ctc_loss(y_true, y_pred):\n",
        "    # Compute label lengths (for CTC)\n",
        "    label_length = tf.math.count_nonzero(y_true, axis=1)  # Assuming padding for labels\n",
        "    logit_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1])  # Logit length based on model output\n",
        "    # Compute the CTC loss\n",
        "    loss = tf.nn.ctc_loss(\n",
        "        labels=y_true,\n",
        "        logits=y_pred,\n",
        "        label_length=label_length,\n",
        "        logit_length=logit_length,\n",
        "        blank_index=-1,  # Assuming the last index is reserved for the blank token in CTC\n",
        "        logits_time_major=False  # Set to False if time dimension is not the first\n",
        "    )\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "model_asr.compile(optimizer=optimizer, loss= tf.keras.losses.CategoricalCrossentropy())\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model_asr.fit(xtrain, ytrain, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0471f6b9-d287-49c7-85c7-82eed5e436d5",
      "metadata": {
        "id": "0471f6b9-d287-49c7-85c7-82eed5e436d5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd71709-d524-4883-8da8-e1e86f6783aa",
      "metadata": {
        "id": "fcd71709-d524-4883-8da8-e1e86f6783aa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5478d17-07b7-4f99-be9f-c1023df0c810",
      "metadata": {
        "id": "f5478d17-07b7-4f99-be9f-c1023df0c810",
        "outputId": "bffacbb4-b5df-4e26-9ab5-60e1a5e739e6",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Shape of y_pred (logits): (None, 98, 30)\n",
            "Shape of y_true (labels): (None,)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_858/334786748.py\", line 71, in ctc_loss_fn  *\n        label_length = tf.reduce_sum(tf.cast(tf.not_equal(y_true, 0), tf.int32), axis=-1)\n\n    TypeError: Expected string passed to parameter 'y' of op 'NotEqual', got 0 of type 'int' instead. Error: Expected string, but got 0 of type 'int'.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[53], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(), loss\u001b[38;5;241m=\u001b[39mctc_loss_fn)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# For demonstration purposes, placeholders are used for `train_dataset`.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/tmp/__autograph_generated_file8hczql6s.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/tmp/__autograph_generated_filer_1a7b0j.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__ctc_loss_fn\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     11\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of y_true (labels):\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(y_true)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m input_length \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfill, ([ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m0\u001b[39m]], ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 13\u001b[0m label_length \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnot_equal, (ag__\u001b[38;5;241m.\u001b[39mld(y_true), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mint32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)), fscope)\n\u001b[1;32m     14\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput length:\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(input_length))\n\u001b[1;32m     15\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel length:\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(label_length))\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_858/334786748.py\", line 71, in ctc_loss_fn  *\n        label_length = tf.reduce_sum(tf.cast(tf.not_equal(y_true, 0), tf.int32), axis=-1)\n\n    TypeError: Expected string passed to parameter 'y' of op 'NotEqual', got 0 of type 'int' instead. Error: Expected string, but got 0 of type 'int'.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset (for example, SpeechCommands)\n",
        "# data_dir = \"path_to_your_audio_dataset\"\n",
        "# train_ds = dataset1['train']['audio']\n",
        "\n",
        "# Parameters\n",
        "batch_size = 32\n",
        "num_mfcc = 13\n",
        "num_classes = 29  # Characters in alphabet + space + blank token\n",
        "\n",
        "# Audio preprocessing function (e.g., converting to MFCC features)\n",
        "def preprocess_audio(audio, sample_rate=16000):\n",
        "    audio = tf.squeeze(audio, axis=-1)  # Remove last axis\n",
        "    audio = tf.cast(audio, tf.float32)  # Normalize\n",
        "\n",
        "    # Compute Short-time Fourier Transform (STFT)\n",
        "    stft = tf.signal.stft(audio, frame_length=400, frame_step=160, fft_length=512)\n",
        "    spectrogram = tf.abs(stft)\n",
        "\n",
        "    # Compute Mel-scaled spectrogram\n",
        "    num_mel_bins = 40\n",
        "    lower_edge_hertz, upper_edge_hertz = 80.0, 4000.0\n",
        "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "        num_mel_bins, spectrogram.shape[-1], sample_rate, lower_edge_hertz, upper_edge_hertz)\n",
        "    mel_spectrogram = tf.tensordot(spectrogram, linear_to_mel_weight_matrix, 1)\n",
        "    mel_spectrogram.set_shape(spectrogram.shape[:-1].concatenate([num_mel_bins]))\n",
        "\n",
        "    # Compute log-mel spectrogram\n",
        "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
        "\n",
        "    # Compute MFCCs from log-mel spectrogram\n",
        "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)[..., :num_mfcc]\n",
        "\n",
        "    return mfccs\n",
        "\n",
        "# Function to create dataset from audio and transcription files\n",
        "def create_dataset(audio_files, transcripts, batch_size=32):\n",
        "    def load_data(audio_file, transcript):\n",
        "        audio = tf.io.read_file(audio_file)\n",
        "        audio, _ = tf.audio.decode_wav(audio, desired_channels=1, desired_samples=16000)\n",
        "        audio = preprocess_audio(audio)\n",
        "        return audio, transcript\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((audio_files, transcripts))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.padded_batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "# Create your dataset here (audio_files and transcripts need to be preloaded)\n",
        "audio_files = ['audio/audio_1.mp3', 'audio/audio_2.mp3', './audio_3.mp3', './audio_4.mp3', './audio_5.mp3', './audio_6.mp3', './audio_7.mp3', './audio_8.mp3', './audio_9.mp3', './audio_10.mp3', './audio_11.mp3']  # List of audio file paths\n",
        "transcripts = ['transcripts/transcript1.txt', 'transcripts/transcript2.txt', 'transcripts/transcript3.txt', 'transcripts/transcript4.txt','transcripts/transcript5.txt', 'transcripts/transcript6.txt', 'transcripts/transcript7.txt', 'transcripts/transcript8.txt', 'transcripts/transcript9.txt', 'transcripts/transcript10.txt', 'transcripts/transcript11.txt']  # List of corresponding transcriptions\n",
        "train_dataset = create_dataset(audio_files, transcripts)\n",
        "\n",
        "# Build a simple model with CTC loss\n",
        "def build_asr_model(input_dim, output_dim):\n",
        "    inputs = tf.keras.Input(shape=(None, input_dim))  # Time dimension is variable\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(inputs)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.Dense(output_dim + 1, activation='softmax')(x)  # +1 for blank token\n",
        "    model = tf.keras.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "def ctc_loss_fn(y_true, y_pred):\n",
        "    print(\"Shape of y_pred (logits):\", y_pred.shape)  # Model output shape\n",
        "    print(\"Shape of y_true (labels):\", y_true.shape)  # Ground truth label shape\n",
        "\n",
        "    input_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1])\n",
        "    label_length = tf.reduce_sum(tf.cast(tf.not_equal(y_true, 0), tf.int32), axis=-1)\n",
        "\n",
        "    print(\"Input length:\", input_length)  # Should be the same length as y_pred's time dimension\n",
        "    print(\"Label length:\", label_length)  # Should be the actual lengths of the labels (excluding padding)\n",
        "\n",
        "    loss = tf.nn.ctc_loss(\n",
        "        labels=y_true,\n",
        "        logits=y_pred,\n",
        "        label_length=label_length,\n",
        "        logit_length=input_length,\n",
        "        logits_time_major=False,\n",
        "        blank_index=-1\n",
        "    )\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "# Compile the model\n",
        "input_dim = num_mfcc\n",
        "output_dim = num_classes\n",
        "model = build_asr_model(input_dim, output_dim)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=ctc_loss_fn)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, epochs=10)\n",
        "\n",
        "# For demonstration purposes, placeholders are used for `train_dataset`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04db36f",
      "metadata": {
        "id": "a04db36f"
      },
      "source": [
        "### Speech 2 Text Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae82cf46",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "840088b45af44709ad26d9f588a2c5ee"
          ]
        },
        "id": "ae82cf46",
        "outputId": "05951b86-af49-40ae-c247-c400e0292dcc",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "840088b45af44709ad26d9f588a2c5ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "\n",
        "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "\n",
        "model_quantized = torch.quantization.quantize_dynamic(\n",
        "    model,  # Model to quantize\n",
        "    {torch.nn.Linear},  # Layers to quantize (Linear layers in this case)\n",
        "    dtype=torch.qint8,\n",
        "    inplace= True\n",
        ")\n",
        "\n",
        "# Ensure padding is enabled to avoid shape issues\n",
        "inputs = processor(\n",
        "    dataset1['train'][\"audio\"],\n",
        "    sampling_rate=16000,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True  # Add padding so that all input sequences have the same length\n",
        ")\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for faster inference\n",
        "    generated_ids = model.generate(\n",
        "        inputs[\"input_features\"],\n",
        "        attention_mask=inputs[\"attention_mask\"]\n",
        ")\n",
        "\n",
        "# # Now pass the input features and attention mask to the model for generation\n",
        "# generated_ids = model.generate(\n",
        "#     inputs[\"input_features\"],\n",
        "#     attention_mask=inputs[\"attention_mask\"]\n",
        "\n",
        "# Decode the generated ids to transcriptions\n",
        "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "transcription\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379f2c4e-052b-4b4b-9a6f-f4110d638bd1",
      "metadata": {
        "id": "379f2c4e-052b-4b4b-9a6f-f4110d638bd1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b1bddbb2",
      "metadata": {
        "id": "b1bddbb2"
      },
      "source": [
        "### Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993981e6",
      "metadata": {
        "id": "993981e6"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "# Load pre-trained processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Preprocess dataset to get audio input features\n",
        "def preprocess_for_model(examples):\n",
        "    audio = examples[\"audio\"]\n",
        "    inputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n",
        "    inputs[\"labels\"] = torch.tensor(examples[\"label\"], dtype=torch.long)  # Ensure labels are Long\n",
        "    return inputs\n",
        "\n",
        "# Apply the preprocessing to your dataset\n",
        "encoded_dataset = dataset.map(preprocess_function, remove_columns=[\"audio\"], batched=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb5aa95",
      "metadata": {
        "id": "5fb5aa95"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# Load pre-trained Wav2Vec2 model for audio classification\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base-960h\",\n",
        "    num_labels=1  # 2 classes: speech and music\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc8ae57",
      "metadata": {
        "id": "bbc8ae57"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./wav2vec2_output\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Create the trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "\n",
        "    tokenizer=processor.feature_extractor,  # Use the Wav2Vec2 processor for tokenizing\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d04eca",
      "metadata": {
        "id": "b6d04eca"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d93daec0",
      "metadata": {
        "id": "d93daec0"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
        "\n",
        "# Load the model and processor\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"./saved_model\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"./saved_model\")\n",
        "\n",
        "# Preprocess new audio for inference\n",
        "def predict(audio_path):\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "    return predicted_class\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/path/to/your/new_audio.wav\"\n",
        "prediction = predict(audio_path)\n",
        "print(\"Prediction:\", \"Speech\" if prediction == 0 else \"Music\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5af834e",
      "metadata": {
        "id": "a5af834e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "775d05d2",
      "metadata": {
        "id": "775d05d2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56701a5",
      "metadata": {
        "id": "f56701a5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}