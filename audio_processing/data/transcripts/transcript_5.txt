Mean square error cost function, gradient descent and learning rate. These are some of the important concepts in machine learning, and that's what we are going to cover today. At the end of this tutorial, we would have written a python program to implement gradient descent. Now, when you start going through machine learning tutorials, the thing that you inevitably come across is mathematical equations. And by looking at them, the first thought that jumps into your mind is, oh my God, I suck at math. I used to get four out of 50 in my math test. How am I going to deal with this? Let's not worry too much about it. We can take one step at a time and the things won't seem that much hard. Also, you won't be implementing gradient descent when solving the actual machine learning problem. But the reason we are doing this exercise today is that you should know some of the internals, so that while using the Sklearn library, you know what's going on, and you can use the libraries in a better way with that. Let's get started. During our linear algebra class in our school days, what we used to have was this equation and x as an input. And we used to compute the value of y, where the way you derive nine is by multiplying this three with two, which will be six plus three. And that's how you come up with nine. In case of machine learning, however, you have observations or training data set, which is your input and output. Using that, you try to derive an equation, also known as prediction function, so that you can use this equation to predict future values of x. In case of our problem of predicting home prices, we saw in the initial linear regression tutorials that we have area and price. And using that, we came up with this equation. Now, if you don't know this equation quite well, you look at my Jupyter notebook, and in that notebook, you'll see that I have this coefficient and intercept. Okay? So that's what I have put it here. So, here I have the home prices in Monroe township. And. And I have plotted these on this chart. And what we are going to look at is, how can you derive this equation given this input and output? Okay, so our goal is to derive this equation. That equation is nothing but this blue line, which is the best fit line, going through all these data points right now, these data points are scattered, so it's not possible to draw the perfect line. But you try to draw a line, which is kind of a best fit. All right? But the problem here is you might have so many lines, right, that can potentially go through these data points. My data set is very simple here. If you have like very heavy data set and if it's, it's like scattered all over the place, then drawing these lines becomes even more difficult. So how do you know which of these lines is the best fit line? Okay, so that's the problem that we are going to solve today. So, one way is you draw any random line. Then from your actual data point, you calculate the error between that data point and the data point predicted by your line. Okay? So call it a delta. You collect all these deltas and square them. The reason you want to square them is these deltas could be negative also. And if you don't square them and just add them, then the results might be skewed. After that, you sum them up and divide it by n. So n here is five, it is number of data points that you have available. The result is called mean square error. Okay? And mean square error is nothing but your actual data point minus the predicted data point. You square it, sum them up, and then divide by n. This mean square error is also called a cost function. There are different type of cost function as well, but mean square error is the most popular one. And here y predicted is replaced by mx plus b because you know that y is equal to mx plus b. So that's the equation for mean square error. Now, we initially saw that there are so many different lines that you can draw. You're not going to try every permutation and combination of m and b because that is very inefficient. You want to take some efficient approach where in very less iteration you can reach your answer. Okay, and gradient descent is that algorithm that helps you find the best fit line in very less number of iteration or in a very efficient way. Okay, so we are going to look at how gradient descent works. Now for that I have plotted m b against the mean square error or a cost function. So here I have drawn different values of m, and here there are different values of pknorthenne, which is your intercept. And for every value of m and b, you will find some cost. So if you keep on plotting those points here, and if you create a plane out of it, it will look like this, it will be like a ball. And what you want to do is you want to start with some value of m and b. People usually start with zero. So here you can see this point has m as zero and b as zero. And from that point you calculate the cost. So let's say the cost is thousand, then you reduce the value of m and b by some amount and we'll see what that amount is later on. So you take kind of like a mini step. You come here and you will see that the error is now reduced to somewhere around 900 and something like that. And then again you reduce it by taking one more step. You keep on taking these steps until you reach this point, which is your minima. Here the error is minimum. And once you reach that you have found your answer because you will use that m and b in your prediction function. I have plotted these different lines and these different lines will have different values of m and b. Let's say this orange has m one, b one. So that m one, b one will be here somewhere. Blue line will have m two, b two, m two, b two will be this red dot. Then red line will have m three, b three. So m three, b three will be somewhere here on this plot. So you can have like so many numerous lines which can create this plot. Now we just said that you will take these baby step, but how exactly you do it? Because visually it sounds easy, but mathematically when you give this task to your computer, you have to come up with some concrete approach. Alright, so we'll look into that. But here is the nice visualization of how you can reduce your m and b and reach the best fit line. Okay. All right, so let's look at how you're going to take those baby steps. So I have these two charts. If you look at this 3d plot from this direction, what you will see is a chart of b against cost and that will be this curvature. Similarly, if you look at this chart from this direction, the chart will look something like this. And in both the cases you are starting at this point, which is this star, and then taking these mini steps and trying to reach this minimum point which is this red dot. Now how do you take these steps? Right. So one approach is, let's say you take fixed size steps. So here if you plot this against b, I am taking fixed size steps on b. But the problem that it can create is by taking these steps I might miss the global minima. Okay, I might just miss it. And my gradient descent will never converge it from this point. It will just start going up and up and I don't know where is my minima. Alright, so this approach is not gonna work. What can work is if you take steps like this. So here in each step I am following the curvature of my chart. And also as I reach near to my red point, you can see that the step size is reducing. You see like this arrow is bigger and these arrows are getting smaller and smaller. If I do something like this, then I can reach this minima. Now how do I do that? So, at each point you need to calculate the slope. Okay? So for example, at this point, the slope will be this. This is a tangent at the curvature, and at this point here, my slope will be this. Once I have a slope, I know which direction I need to go in. For example, if you look at this green line and if I am at this blue dot, I know that I need to go in this direction. And then there is something called a learning rate, which you can use in conjunction with this slope here to take that step and reach the next point. Now we have to get into calculus a little bit, because calculus allows you to figure out these baby steps. And when we are talking about these slopes, really, this slope is nothing but a derivative of b with respect to this cost function. Okay, if you want to go in details, I recommend this channel three blue, one, brown. This guy is very good in explaining mathematical concept using a nice visualization, so you will really find it very useful and pleasing. But if you don't want to go in details, then in this tutorial I'll just quickly walk over some of the basic concepts. Okay, let's look at what is derivative. So derivatives is all about slope. I'm on this website called math is fun and these guys have explained it really well. So, slope is nothing but a change in Y divided by change in x. Okay, so if you have line like this, and if you want to calculate slope between the two points here it is 24 by 15. But what if you want to calculate the slope at a particular point, right? Like in our case, if you remember here, we want to calculate a slope at a particular point. Same thing here, right? So that slope will be nothing but a small change in y divided by small change in xdev. Alright, we'll say as x shrinks to zero and y shrinks to zero, that's when you get more accurate slope. Okay, so for the equation like x square, that slope will be two x. Okay, this is again called a derivative. Derivative is mentioned by this notation, d by dx. The derivative of x square is two x. So for example, for this chart, the slope here is four, because this is x square. And the value here for the slope will be four. Okay, now let's look at what is partial derivative. When you have an equation like this, where you have your function depending on two variables, x and y. What you try to do is you calculate a partial derivative of x. In that case, you keep the y zero. So here f dot x is nothing but a partial derivative of x square with respect to x. Okay? Similarly, when you want to calculate a partial derivative of this function with respect to y, what you do is you keep x zero and then you calculate the derivative of y. Alright? And general rule here is, let's say if you have y cube, right? Like how did you come up with three y squared? You put three here in front of y and then you subtract one from three. So that's how you get three y square. Okay? So those are like some of the basic concept of derivative and partial derivatives. Again, if you want to go in detail, just follow a three blue one, brown YouTube channel. And that guy is really good in explaining these concepts in detail. Okay, so just to revise the concept, the derivative of this function will be three x square. And this is the notation for your derivative, the derivative of functions which has dependency on two variables. It will be a partial derivative, and a partial derivative of this function with respect to x will be this, and with respect to y will be two y. And this is how you mention your derivative. This is the notation that you use, right? It's short of looks like a d, but it's not like it's like a curved d. So now going back to our problem of the line, right? So here we want to find the partial derivative of B. And then for the other char, we want to find the partial derivative of M. Okay? So how do you find that? So this is your mean square error function. And the partial derivative of M will be this. And partial derivative of B will be this. Now, I'm not going to go in detail about how we came about this. Again, you can follow other resources, but you can just accept this equation. It's sort of like a rule, you know, it's like why earth rotates around the sun, or why humans have two eyes. Well, it's just a law of nature. But if you want to go in detail on how we derived this, then you can follow some other resources. But one hint I can give you is seed. This thing has a square. So generally for a derivative, you put two here. So two chem here, and this becomes two minus one, which is one, which we don't show here. Alright? And then once you have partial derivative, what you are having is a direction. So partial derivatives gives you a slope. And then once you have direction, now you need to take a step. So for the step, you use something called a learning rate. Alright? So you have initial value of M and then you subtract this much, your learning rate into slope. So for example, you are here on this chart. This is your b one value. To come up with this b two value, you will subtract learning rate multiplied by the partial derivative, which is nothing but a slope here. Now let's write Python code to implement gradient descent. I am going to use Pycharm today instead of Jupyter notebook because I am planning to use some of the debugging features. Pycharm's community edition is freely available to download from jetbrains website. The problem we are solving here is we have a value of x and y vectors and we want to derive the best fit line or an equation using m and b. So you have x and y and you want to come up with correct value of m and b. Alright, so that's our objective here. I'm going to use a numpy array instead of simple python list because matrix multiplication is very convenient with this. And also numpy array tends to be more faster than simple python list. So the first thing we are going to do is start with some value of m current and b current. Right? So again, to revise the theory, you start with some value of m and b. And then you take these baby steps to reach to a global minima. So as you can see in the chart, we started with m and b values as being zero. And then we took these steps one by one to reach the global minima. Another thing you need to do is define the number of iterations you have to define how many bb steps you are going to do. I am going to start with 1000 and then I will fine tune it. Okay? Again, all of this is pretty much like a trial and error. I will start with some parameters and I will see how my algorithm behaves and then I will fine tune them. Alright, so let's run a for loop, simple for loop, which just iterates these many iterations. And at each step what you do is first thing is you calculate the predicted value of y. Alright, so y predicted is nothing but m current into x plus b current. All right, pretty straightforward. Y is equal to mx plus b. And for the assumptions that you have for m and b, you are calculating y, which is y predicted. Next step is to calculate m derivative and b derivative. So m derivative, I'm going to call it mdd and the equation is two by n. All right, now what is n? N is the length of these data points. I'm assuming x and y's length is same if it is not the case, then you can add necessary validation and throw an error. All right, so the m's derivative is minus two by n multiplied by sum of something. What is that sum? Sum is x multiplied by y minus y predicted. Okay, so y minus y predicted and b's derivatives. Equation is same, except the fact that it doesn't have this x multiplication. Once you have that, you're going to adjust your m current as shown in the equation. Your next m will be your current m minus learning rate into m derivative. So we have m derivative, but we need learning rate. Okay, so I'm going to define learning rate now. Okay, again, this is a parameter that you have to start with some value. So I'm going to start with 0.001. People generally start with zero, zero, one, and then they gradually improve it. You can remove zeros, you can, you can use like five, whatever. This is like trial and error. You see how your algorithm behaves and then you tweak those parameters. Similarly for b, the equation is the value of b is b minus learning rate into your partial derivative. And then at each iteration I will print their values. Let me print iteration also so that I know what's going on at each iteration. Okay. All right, so looks like my code is good enough and I can just run it. So right click and run it. Okay, so let's see what happened. So you can see that we started with some value of m and b, and now in the end we are at 2.44 and 1.38. Okay, now if you want to know how well you are doing, you need to print cost. At each iteration. You should be reducing your cost, right? So if you remember that 3d diagram, at each step, you should be reducing your cost. Sometimes if you don't write your program well, and if you start increasing the cost, then you are never going to find the answer. So, let's print cost. So what is a cost? Cost is equal to. If you check the equation, it is one divided by n multiplied by sum of something. And that something is all the data point differences between y and y predicted and their square. So we need a list here, and the value in each of the, the list element is this. You will run a for loop on for value in y minus y predicted. And I'm using a list comprehension here. And for each of these values you want to take their square. And this is to deal with the negative values. After that, I will print the cost at each iteration. And when I run this, I can now track down the cost. You can see the cost is reducing in each of these steps. Now, how do I know when I need to stop so I can keep on increasing my iterations, and you can see that I'm getting closer and closer to my expected m and b value, which is two and three. You can also manipulate your learning rate. So what I usually like to do is first I will keep the iterations less. I will start with some learning rate, and I will see if I am reducing the cost on each iterations. So here with this learning rate, I can see that I am reducing my cost. Okay, so I can maybe take even a bigger step. So I will do probably zero. And let's see what happens with that now here. Yes. So with that also I am reducing my cost. So I think this is fine. Let me try one more bigger step, which is this. Now you can see that I started increasing my cost, so this learning rate is too big. I am crossing my global minima and I am shooting in the other direction. So I have to be between 0.01 and 0.1. So how about 0.09 there? Also I am increasing, so maybe 0.08. Okay, that looks good. Here I am reducing. Alright, so I will stick with this learning rate and increase my iterations to let's say 10,000. You can see that now I reached my optimum value. The expected value of m was two and b was three. So which is almost near to three. And you can see the cost is very minute. This is how you can approach your gradient descent algorithm and stop whenever you reach some threshold of cost. Or even you can compare the cost between different iteration and you can find out, see the, the property of this curvature is that once you reach the global minima, your cost will kind of stay the same. Okay, if you are using the correct learning rate. So here you see in all these iteration, your cost is almost remaining constant. So you can use floating point comparison and just compare to cost and stop whenever your cost is not reducing too much. I also have a visual representation of how my MNB is moving towards the best fit line. So we started here and then we gradually we were moving closer to those points. Though those red points are not quite visible, but they are here, here. And you can see that gradually I am reaching more and more closer towards those points. So you can use this Jupyter notebook for visualization purpose. And now we'll move into our exercise section. So the problem that you have to solve today is you are given the mathematics and computer science test scores for all these students and you have to find out the correlation between the math score and computer science score. So in summary, b is your x and computer science score, which is column c as your y. Using this you will find a value of m and b by applying gradient descent algorithm. And what you have to do is you have to compare the cost between each iteration and when it is within certain threshold. And to compare the threshold we are going to use a math dot is close function and use a tolerance of one e raised to -20 okay, so if your two costs are in this range then you have to stop your for loop and you have to tell me how many iteration you need to figure out the value of m and b.